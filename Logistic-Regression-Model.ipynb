{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4704b9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.14.tar.gz (82 kB)\n",
      "     -------------------------------------- 82.1/82.1 kB 574.8 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Collecting certifi>=2023.7.22\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "     -------------------------------------- 164.4/164.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (1.26.11)\n",
      "Requirement already satisfied: bleach in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from bleach->kaggle) (21.3)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging->bleach->kaggle) (3.0.9)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.14-py3-none-any.whl size=105123 sha256=70f230114098b31f7acb0ccda665d5210b578022ed43857b830ca9166ec48d34\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\23\\f1\\ec\\3f9079ada5cc1218f9accf05ccca9f0a57533191e49d76e3f6\n",
      "Successfully built kaggle\n",
      "Installing collected packages: certifi, kaggle\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.9.14\n",
      "    Uninstalling certifi-2022.9.14:\n",
      "      Successfully uninstalled certifi-2022.9.14\n",
      "Successfully installed certifi-2024.6.2 kaggle-1.6.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Installing the kaggle library\n",
    "pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441db675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "Downloading sentiment140.zip to C:\\Users\\LENOVO\\Desktop\\AI_ML\\Projects\\Sentiment-Analysis\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/80.9M [00:00<?, ?B/s]\n",
      "  1%|1         | 1.00M/80.9M [00:01<02:10, 644kB/s]\n",
      "  2%|2         | 2.00M/80.9M [00:01<01:02, 1.33MB/s]\n",
      "  4%|3         | 3.00M/80.9M [00:01<00:39, 2.09MB/s]\n",
      "  5%|4         | 4.00M/80.9M [00:02<00:28, 2.85MB/s]\n",
      "  6%|6         | 5.00M/80.9M [00:02<00:22, 3.59MB/s]\n",
      "  7%|7         | 6.00M/80.9M [00:02<00:18, 4.17MB/s]\n",
      "  9%|8         | 7.00M/80.9M [00:02<00:16, 4.78MB/s]\n",
      " 10%|9         | 8.00M/80.9M [00:02<00:14, 5.17MB/s]\n",
      " 11%|#1        | 9.00M/80.9M [00:02<00:13, 5.52MB/s]\n",
      " 12%|#2        | 10.0M/80.9M [00:03<00:12, 5.79MB/s]\n",
      " 14%|#3        | 11.0M/80.9M [00:03<00:12, 5.96MB/s]\n",
      " 15%|#4        | 12.0M/80.9M [00:03<00:11, 6.04MB/s]\n",
      " 16%|#6        | 13.0M/80.9M [00:03<00:11, 6.37MB/s]\n",
      " 17%|#7        | 14.0M/80.9M [00:03<00:10, 6.43MB/s]\n",
      " 19%|#8        | 15.0M/80.9M [00:03<00:10, 6.41MB/s]\n",
      " 20%|#9        | 16.0M/80.9M [00:04<00:10, 6.42MB/s]\n",
      " 21%|##1       | 17.0M/80.9M [00:04<00:10, 6.60MB/s]\n",
      " 22%|##2       | 18.0M/80.9M [00:04<00:10, 6.52MB/s]\n",
      " 23%|##3       | 19.0M/80.9M [00:04<00:09, 6.69MB/s]\n",
      " 25%|##4       | 20.0M/80.9M [00:04<00:09, 6.49MB/s]\n",
      " 26%|##5       | 21.0M/80.9M [00:04<00:09, 6.59MB/s]\n",
      " 27%|##7       | 22.0M/80.9M [00:05<00:09, 6.68MB/s]\n",
      " 28%|##8       | 23.0M/80.9M [00:05<00:09, 6.73MB/s]\n",
      " 30%|##9       | 24.0M/80.9M [00:05<00:09, 6.48MB/s]\n",
      " 31%|###       | 25.0M/80.9M [00:05<00:09, 6.51MB/s]\n",
      " 32%|###2      | 26.0M/80.9M [00:05<00:08, 6.57MB/s]\n",
      " 33%|###3      | 27.0M/80.9M [00:05<00:08, 6.58MB/s]\n",
      " 35%|###4      | 28.0M/80.9M [00:06<00:09, 6.07MB/s]\n",
      " 36%|###5      | 29.0M/80.9M [00:06<00:09, 5.52MB/s]\n",
      " 37%|###7      | 30.0M/80.9M [00:06<00:08, 5.94MB/s]\n",
      " 38%|###8      | 31.0M/80.9M [00:06<00:08, 6.26MB/s]\n",
      " 40%|###9      | 32.0M/80.9M [00:06<00:08, 6.30MB/s]\n",
      " 41%|####      | 33.0M/80.9M [00:06<00:07, 6.50MB/s]\n",
      " 42%|####2     | 34.0M/80.9M [00:07<00:07, 6.64MB/s]\n",
      " 43%|####3     | 35.0M/80.9M [00:07<00:07, 6.60MB/s]\n",
      " 44%|####4     | 36.0M/80.9M [00:07<00:07, 6.68MB/s]\n",
      " 46%|####5     | 37.0M/80.9M [00:07<00:06, 6.58MB/s]\n",
      " 47%|####6     | 38.0M/80.9M [00:07<00:06, 6.78MB/s]\n",
      " 48%|####8     | 39.0M/80.9M [00:07<00:06, 6.77MB/s]\n",
      " 49%|####9     | 40.0M/80.9M [00:07<00:06, 6.93MB/s]\n",
      " 51%|#####     | 41.0M/80.9M [00:08<00:06, 6.78MB/s]\n",
      " 52%|#####1    | 42.0M/80.9M [00:08<00:05, 6.81MB/s]\n",
      " 53%|#####3    | 43.0M/80.9M [00:08<00:05, 6.88MB/s]\n",
      " 54%|#####4    | 44.0M/80.9M [00:08<00:05, 6.97MB/s]\n",
      " 56%|#####5    | 45.0M/80.9M [00:08<00:05, 6.93MB/s]\n",
      " 57%|#####6    | 46.0M/80.9M [00:08<00:05, 6.94MB/s]\n",
      " 58%|#####8    | 47.0M/80.9M [00:09<00:05, 6.87MB/s]\n",
      " 59%|#####9    | 48.0M/80.9M [00:09<00:04, 6.99MB/s]\n",
      " 61%|######    | 49.0M/80.9M [00:09<00:04, 6.95MB/s]\n",
      " 62%|######1   | 50.0M/80.9M [00:09<00:04, 6.96MB/s]\n",
      " 63%|######3   | 51.0M/80.9M [00:09<00:04, 6.81MB/s]\n",
      " 64%|######4   | 52.0M/80.9M [00:09<00:04, 6.78MB/s]\n",
      " 65%|######5   | 53.0M/80.9M [00:09<00:04, 6.88MB/s]\n",
      " 67%|######6   | 54.0M/80.9M [00:10<00:04, 6.85MB/s]\n",
      " 68%|######7   | 55.0M/80.9M [00:10<00:04, 6.76MB/s]\n",
      " 69%|######9   | 56.0M/80.9M [00:10<00:04, 6.20MB/s]\n",
      " 70%|#######   | 57.0M/80.9M [00:10<00:03, 6.32MB/s]\n",
      " 72%|#######1  | 58.0M/80.9M [00:10<00:03, 6.41MB/s]\n",
      " 73%|#######2  | 59.0M/80.9M [00:10<00:03, 6.60MB/s]\n",
      " 74%|#######4  | 60.0M/80.9M [00:11<00:03, 6.24MB/s]\n",
      " 75%|#######5  | 61.0M/80.9M [00:11<00:03, 6.74MB/s]\n",
      " 77%|#######6  | 62.0M/80.9M [00:11<00:02, 6.91MB/s]\n",
      " 78%|#######7  | 63.0M/80.9M [00:11<00:02, 6.95MB/s]\n",
      " 79%|#######9  | 64.0M/80.9M [00:11<00:02, 7.03MB/s]\n",
      " 80%|########  | 65.0M/80.9M [00:11<00:02, 7.09MB/s]\n",
      " 82%|########1 | 66.0M/80.9M [00:11<00:02, 7.10MB/s]\n",
      " 83%|########2 | 67.0M/80.9M [00:12<00:02, 7.11MB/s]\n",
      " 84%|########4 | 68.0M/80.9M [00:12<00:01, 7.00MB/s]\n",
      " 85%|########5 | 69.0M/80.9M [00:12<00:01, 7.00MB/s]\n",
      " 87%|########6 | 70.0M/80.9M [00:12<00:01, 7.05MB/s]\n",
      " 88%|########7 | 71.0M/80.9M [00:12<00:01, 7.02MB/s]\n",
      " 89%|########8 | 72.0M/80.9M [00:12<00:01, 7.00MB/s]\n",
      " 90%|######### | 73.0M/80.9M [00:12<00:01, 7.05MB/s]\n",
      " 91%|#########1| 74.0M/80.9M [00:13<00:01, 7.09MB/s]\n",
      " 93%|#########2| 75.0M/80.9M [00:13<00:00, 7.05MB/s]\n",
      " 94%|#########3| 76.0M/80.9M [00:13<00:00, 7.00MB/s]\n",
      " 95%|#########5| 77.0M/80.9M [00:13<00:00, 7.04MB/s]\n",
      " 96%|#########6| 78.0M/80.9M [00:13<00:00, 7.00MB/s]\n",
      " 98%|#########7| 79.0M/80.9M [00:13<00:00, 6.78MB/s]\n",
      " 99%|#########8| 80.0M/80.9M [00:14<00:00, 6.93MB/s]\n",
      "100%|##########| 80.9M/80.9M [00:14<00:00, 6.94MB/s]\n",
      "100%|##########| 80.9M/80.9M [00:14<00:00, 5.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "# API to data fetch from kaggle\n",
    "!kaggle datasets download -d kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4fdf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is extracted\n"
     ]
    }
   ],
   "source": [
    "# Extracting the compress data set\n",
    "from zipfile import ZipFile\n",
    "dataset = './sentiment140.zip'\n",
    "\n",
    "with ZipFile(dataset, 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('The dataset is extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b7ea0",
   "metadata": {},
   "source": [
    "# Import the necessary libarary and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff7528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104aef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef16b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#print stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271d7d1",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caf13271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the csv filej to pandas datafram\n",
    "twitter_data=pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding = 'ISO-8859-1')\n",
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd045eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0          scotthamilton   \n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bc4fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naming the columns and read the data again\n",
    "column_names=['sentiment', 'id', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# loading the csv filej to pandas datafram\n",
    "twitter_data=pd.read_csv('./training.1600000.processed.noemoticon.csv', names=column_names, encoding = 'ISO-8859-1')\n",
    "twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361eb1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "id           0\n",
       "date         0\n",
       "flag         0\n",
       "user         0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the missing or null value\n",
    "twitter_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab917946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "4    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distibution of target column\n",
    "twitter_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4a5e230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    800000\n",
       "1    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the '4' to '1' \n",
    "# 0---> -ve sentment\n",
    "# 1---> +ve sentiment\n",
    "twitter_data.replace({'sentiment':{4:1}}, inplace=True) # inplace is change the original data\n",
    "\n",
    "# now the distibution of target column is\n",
    "twitter_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c089c5",
   "metadata": {},
   "source": [
    "## Stemming:\n",
    "\n",
    "The process of reducing the words it it's root form eg: actor, acting, actoress---> act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "830b6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e52ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split() # now the stemmed_content is list that contains all words of a tweet text\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    \n",
    "    return stemmed_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "739b8a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['stemmed_content']= twitter_data['text'].apply(stemming) #It will take almost 50 minutes to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568b300",
   "metadata": {},
   "source": [
    "twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff51e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          switchfoot http twitpic com zl awww bummer sho...\n",
      "1          upset updat facebook text might cri result sch...\n",
      "2          kenichan dive mani time ball manag save rest g...\n",
      "3                            whole bodi feel itchi like fire\n",
      "4                              nationwideclass behav mad see\n",
      "                                 ...                        \n",
      "1599995                           woke school best feel ever\n",
      "1599996    thewdb com cool hear old walt interview http b...\n",
      "1599997                         readi mojo makeov ask detail\n",
      "1599998    happi th birthday boo alll time tupac amaru sh...\n",
      "1599999    happi charitytuesday thenspcc sparkschar speak...\n",
      "Name: stemmed_content, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['stemmed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e27e79c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          0\n",
      "2          0\n",
      "3          0\n",
      "4          0\n",
      "          ..\n",
      "1599995    1\n",
      "1599996    1\n",
      "1599997    1\n",
      "1599998    1\n",
      "1599999    1\n",
      "Name: sentiment, Length: 1600000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(twitter_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d0b20c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['switchfoot http twitpic com zl awww bummer shoulda got david carr third day',\n",
       "       'upset updat facebook text might cri result school today also blah',\n",
       "       'kenichan dive mani time ball manag save rest go bound', ...,\n",
       "       'readi mojo makeov ask detail',\n",
       "       'happi th birthday boo alll time tupac amaru shakur',\n",
       "       'happi charitytuesday thenspcc sparkschar speakinguph h'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating the data and label\n",
    "x=twitter_data['stemmed_content'].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c313911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=twitter_data['sentiment'].values\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bbabb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3bb4525",
   "metadata": {},
   "source": [
    "### Splitting the data to training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fca47781",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "718f876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000,) (1280000,) (320000,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "398e3366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mmangen fine much time chat twitter hubbi back summer amp tend domin free time'\n",
      " 'ah may show w ruth kim amp geoffrey sanhueza'\n",
      " 'ishatara mayb bay area thang dammit' ...\n",
      " 'destini nevertheless hooray member wonder safe trip' 'feel well'\n",
      " 'supersandro thank']\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602421a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9828093e",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    " converting the textual data to numerical data using TFIDF(Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a339f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d071e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 443066)\t0.4484755317023172\n",
      "  (0, 235045)\t0.41996827700291095\n",
      "  (0, 109306)\t0.3753708587402299\n",
      "  (0, 185193)\t0.5277679060576009\n",
      "  (0, 354543)\t0.3588091611460021\n",
      "  (0, 436713)\t0.27259876264838384\n",
      "  (1, 160636)\t1.0\n",
      "  (2, 288470)\t0.16786949597862733\n",
      "  (2, 132311)\t0.2028971570399794\n",
      "  (2, 150715)\t0.18803850583207948\n",
      "  (2, 178061)\t0.1619010109445149\n",
      "  (2, 409143)\t0.15169282335109835\n",
      "  (2, 266729)\t0.24123230668976975\n",
      "  (2, 443430)\t0.3348599670252845\n",
      "  (2, 77929)\t0.31284080750346344\n",
      "  (2, 433560)\t0.3296595898028565\n",
      "  (2, 406399)\t0.32105459490875526\n",
      "  (2, 129411)\t0.29074192727957143\n",
      "  (2, 407301)\t0.18709338684973031\n",
      "  (2, 124484)\t0.1892155960801415\n",
      "  (2, 109306)\t0.4591176413728317\n",
      "  (3, 172421)\t0.37464146922154384\n",
      "  (3, 411528)\t0.27089772444087873\n",
      "  (3, 388626)\t0.3940776331458846\n",
      "  (3, 56476)\t0.5200465453608686\n",
      "  :\t:\n",
      "  (1279996, 390130)\t0.22064742191076112\n",
      "  (1279996, 434014)\t0.2718945052332447\n",
      "  (1279996, 318303)\t0.21254698865277746\n",
      "  (1279996, 237899)\t0.2236567560099234\n",
      "  (1279996, 291078)\t0.17981734369155505\n",
      "  (1279996, 412553)\t0.18967045002348676\n",
      "  (1279997, 112591)\t0.7574829183045267\n",
      "  (1279997, 273084)\t0.4353549002982409\n",
      "  (1279997, 5685)\t0.48650358607431304\n",
      "  (1279998, 385313)\t0.4103285865588191\n",
      "  (1279998, 275288)\t0.38703346602729577\n",
      "  (1279998, 162047)\t0.34691726958159064\n",
      "  (1279998, 156297)\t0.3137096161546449\n",
      "  (1279998, 153281)\t0.28378968751027456\n",
      "  (1279998, 435463)\t0.2851807874350361\n",
      "  (1279998, 124765)\t0.32241752985927996\n",
      "  (1279998, 169461)\t0.2659980990397061\n",
      "  (1279998, 93795)\t0.21717768937055476\n",
      "  (1279998, 412553)\t0.2816582375021589\n",
      "  (1279999, 96224)\t0.5416162421321443\n",
      "  (1279999, 135384)\t0.6130934129868719\n",
      "  (1279999, 433612)\t0.3607341026233411\n",
      "  (1279999, 435572)\t0.31691096877786484\n",
      "  (1279999, 31410)\t0.248792678366695\n",
      "  (1279999, 242268)\t0.19572649660865402\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c745ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 420984)\t0.17915624523539803\n",
      "  (0, 409143)\t0.31430470598079707\n",
      "  (0, 398906)\t0.3491043873264267\n",
      "  (0, 388348)\t0.21985076072061738\n",
      "  (0, 279082)\t0.1782518010910344\n",
      "  (0, 271016)\t0.4535662391658828\n",
      "  (0, 171378)\t0.2805816206356073\n",
      "  (0, 138164)\t0.23688292264071403\n",
      "  (0, 132364)\t0.25525488955578596\n",
      "  (0, 106069)\t0.3655545001090455\n",
      "  (0, 67828)\t0.26800375270827315\n",
      "  (0, 31168)\t0.16247724180521766\n",
      "  (0, 15110)\t0.1719352837797837\n",
      "  (1, 366203)\t0.24595562404108307\n",
      "  (1, 348135)\t0.4739279595416274\n",
      "  (1, 256777)\t0.28751585696559306\n",
      "  (1, 217562)\t0.40288153995289894\n",
      "  (1, 145393)\t0.575262969264869\n",
      "  (1, 15110)\t0.211037449588008\n",
      "  (1, 6463)\t0.30733520460524466\n",
      "  (2, 400621)\t0.4317732461913093\n",
      "  (2, 256834)\t0.2564939661498776\n",
      "  (2, 183312)\t0.5892069252021465\n",
      "  (2, 89448)\t0.36340369428387626\n",
      "  (2, 34401)\t0.37916255084357414\n",
      "  :\t:\n",
      "  (319994, 123278)\t0.4530341382559843\n",
      "  (319995, 444934)\t0.3211092817599261\n",
      "  (319995, 420984)\t0.22631428606830145\n",
      "  (319995, 416257)\t0.23816465111736276\n",
      "  (319995, 324496)\t0.3613167933647574\n",
      "  (319995, 315813)\t0.28482299145634127\n",
      "  (319995, 296662)\t0.39924856793840147\n",
      "  (319995, 232891)\t0.25741278545890767\n",
      "  (319995, 213324)\t0.2683969144317078\n",
      "  (319995, 155493)\t0.2770682832971668\n",
      "  (319995, 109379)\t0.30208964848908326\n",
      "  (319995, 107868)\t0.3339934973754696\n",
      "  (319996, 438709)\t0.4143006291901984\n",
      "  (319996, 397506)\t0.9101400928717545\n",
      "  (319997, 444770)\t0.2668297951055569\n",
      "  (319997, 416695)\t0.29458327588067873\n",
      "  (319997, 349904)\t0.32484594100566083\n",
      "  (319997, 288421)\t0.48498483387153407\n",
      "  (319997, 261286)\t0.37323893626855326\n",
      "  (319997, 169411)\t0.403381646999604\n",
      "  (319997, 98792)\t0.4463892055808332\n",
      "  (319998, 438748)\t0.719789181620468\n",
      "  (319998, 130192)\t0.6941927210956169\n",
      "  (319999, 400636)\t0.2874420848216212\n",
      "  (319999, 389755)\t0.9577980203954275\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65673178",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "\n",
    "Training the model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c4353a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fdc119a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e07a73",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f645393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score on the training data\n",
    "x_train_prediction=model.predict(x_train)\n",
    "training_data_accuracy = accuracy_score(y_train, x_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "320e8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for the training data = 0.81019765625\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score for the training data =\", training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60c55a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for the test data = 0.778003125\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score on the test data\n",
    "x_test_prediction=model.predict(x_test)\n",
    "test_data_accuracy = accuracy_score(y_test, x_test_prediction)\n",
    "print(\"Accuracy Score for the test data =\", test_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6f073",
   "metadata": {},
   "source": [
    "### Save the train model for the future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39199a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ce6c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'LR_model1.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c0c49",
   "metadata": {},
   "source": [
    "### Using the saved model(pickled file) for the future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6732431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "load_model = pickle.load(open('./LR_model1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fedbdbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "x_new= x_test[200]\n",
    "print(y_test[200])\n",
    "\n",
    "prediction=load_model.predict(x_new)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349401e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c947d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedaadbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
